{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSCI632 - Fake News Detection"
      ],
      "metadata": {
        "id": "bNyxXsbnSRCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Setup"
      ],
      "metadata": {
        "id": "SOWTDB0dS1i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "-SliKjieaN__"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pK5Cef-__h19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac71c41-341f-40a9-924a-e6e1c1d7d286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=0d8144446f0984369cfdf074276e1382ef311216cd22c9e486e286ba3700262f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Cloning into 'DSCI632-Project'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 85 (delta 17), reused 65 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop2.7\"\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "!git clone https://github.com/avivfaraj/DSCI632-Project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important packages"
      ],
      "metadata": {
        "id": "OI1-eTO-SWj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Pandas is required to read the data.\n",
        "# For some reason pyspark can't read the csv file correctly\n",
        "# So we have to read using pandas and then convert to spark DF\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.ml.feature import IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql.functions import when, col, regexp_replace, concat, lit, length\n",
        "from pyspark.sql.types import FloatType, DoubleType\n",
        "from pyspark.ml.classification import NaiveBayesModel, NaiveBayes\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
      ],
      "metadata": {
        "id": "uBp8eJ5wn4fm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(df, labelCol = \"label\", predCol = \"prediction\"):\n",
        "    TP = df.filter((col(labelCol) == 0) & (col(predCol) == 0)).count()\n",
        "    FN = df.filter((col(labelCol) == 1) & (col(predCol) == 0)).count()\n",
        "    FP = df.filter((col(labelCol) == 0) & (col(predCol) == 1)).count()\n",
        "    TN = df.filter((col(labelCol) == 1) & (col(predCol) == 1)).count()\n",
        "\n",
        "    precision = (TP)/(TP+FP)\n",
        "    recall = (TP)/(TP+FN)\n",
        "    print(\"Accuracy: %.3f\" % float((TP+TN)/(TP+TN+FP+FN)))\n",
        "    print(\"Recall: %.3f\" % float(recall))\n",
        "    print(\"Precision: %.3f\" % float(precision))\n",
        "    print(\"F1 Score: %.3f\" % float(2*(precision * recall)/(precision +recall)))\n",
        "\n",
        "    (df\n",
        "        .crosstab('label','prediction')\n",
        "        .withColumnRenamed(\"label_prediction\", \"label\\prediction\")\n",
        "        .orderBy(\"label\\prediction\", asceding = False)\n",
        "        .show()\n",
        "    )\n",
        "\n",
        "    return ([[TP, FP], [FN, TN]], precision, recall)"
      ],
      "metadata": {
        "id": "33jp03OHVhPS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Session \\& Reading Dataset"
      ],
      "metadata": {
        "id": "N_VoOgp5S51X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Final Project\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "mySchema = StructType([ StructField(\"index\", IntegerType(), True)\\\n",
        "                       ,StructField(\"title\", StringType(), True)\\\n",
        "                       ,StructField(\"author\", StringType(), True)\\\n",
        "                       ,StructField(\"text\", StringType(), True)\\\n",
        "                       ,StructField(\"label\", IntegerType(), True)])\n",
        "\n",
        "path = \"./DSCI632-Project/data/train.csv\"\n",
        "pandas_df = pd.read_csv(path, header = 0)\n",
        "\n",
        "spark_df = spark.createDataFrame(pandas_df, schema = mySchema)\n"
      ],
      "metadata": {
        "id": "SJyOb7_9_uVo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "q-KwNJlUTVJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Dataset"
      ],
      "metadata": {
        "id": "RtCeamMcTXlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting all rows that are missing text\n",
        "# df_rmv_nan_text = spark_df.filter(col(\"text\") != \"NaN\")\n",
        "df_rmv_nan_text = spark_df.filter(length(col(\"text\")) > 60)\n",
        "\n",
        "# There are a lot of NaN in the dataset.\n",
        "# Those are Null values in pandas that were\n",
        "# Converted to NaN string in spark df.\n",
        "# Since it is a string, it will not be recognized by na() methods\n",
        "# So, we have to manually change their value:\n",
        "df_no_nan = (df_rmv_nan_text\n",
        "             .withColumn(\"title\", when(col(\"title\") == \"NaN\", \" \")\n",
        "                                            .otherwise(col(\"title\")))\n",
        "             )\n",
        "\n",
        "\n",
        "## NOTE: Later on we will use Tokenizer from PySpark MLlib. This tokenizer\n",
        "##       takes care of converting all characters to lowercase, so it is\n",
        "##       not required in this step.\n",
        "\n",
        "# Remove non-character from title and text\n",
        "df_clean = (df_no_nan\n",
        "\n",
        "                 ## Removing any non-character from title\n",
        "                .withColumn(\"title\", \n",
        "                            regexp_replace(\n",
        "                                col('title'),\n",
        "                                r'[^\\w\\’ ]',''))\n",
        "                \n",
        "                ## Removing any non-character from text\n",
        "                .withColumn(\"text\", \n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[^\\w\\’ ]',''))\n",
        "                \n",
        "                ## Replacing 2 or more whitespaces with 1 whitespace\n",
        "                .withColumn(\"text\", \n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[ ]{2,}',' '))\n",
        "                \n",
        "                ## Replacing 2 or more whitespaces with 1 whitespace\n",
        "                .withColumn(\"title\", \n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[ ]{2,}',' '))\n",
        "                )\n",
        "\n",
        "\n",
        "# Concatenation of title and text when title doesn't appear in text\n",
        "df_combined = (df_clean\n",
        "                    .withColumn('full_text',\n",
        "                                  when(col(\"text\").contains(\n",
        "                                                    concat(col(\"title\"))),\n",
        "                                                    col(\"text\"))\n",
        "                                  \n",
        "                                  .otherwise(concat(col(\"title\"),\n",
        "                                                    lit(\" \"),\n",
        "                                                    col(\"text\"))))\n",
        "                    .select([\"full_text\",\"label\"])\n",
        "                    .withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "                    .dropDuplicates()\n",
        "                )\n",
        "\n",
        "\n",
        "# Clean memory             \n",
        "del df_rmv_nan_text, df_no_nan, df_clean\n",
        "\n",
        "# Sanity Check\n",
        "print(df_combined.count())\n",
        "df_combined.show(7)"
      ],
      "metadata": {
        "id": "AyUnPTcsTdk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66fca30-aaa3-4ba7-a9c4-30b0860446ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20210\n",
            "+--------------------+-----+\n",
            "|           full_text|label|\n",
            "+--------------------+-----+\n",
            "|0 0 AP N1 26 27 1...|  1.0|\n",
            "|GREENBELT Md The ...|  0.0|\n",
            "|The Minnesota off...|  0.0|\n",
            "|GeoEngineering Un...|  1.0|\n",
            "|Following a fight...|  0.0|\n",
            "|The military indu...|  1.0|\n",
            "|Insists Russia De...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Class Balance\n",
        "\n",
        "Still balanced!"
      ],
      "metadata": {
        "id": "z9YAncjjgHV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.groupby(\"label\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZRUdo-tr9MG",
        "outputId": "7f336dcf-a084-412a-aa09-3b3ea88b10c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|10385|\n",
            "|  1.0| 9825|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords"
      ],
      "metadata": {
        "id": "pkusjNkLmrg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "\n",
        "# Sanity Check\n",
        "stopwords_ls[:10]"
      ],
      "metadata": {
        "id": "tf7dYGkSo5IE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7781ac-13ee-4f7d-c98a-725f9700daac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemmer Class"
      ],
      "metadata": {
        "id": "38HHWDngVPsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################### Code Citation #######################\n",
        "# Author: Clare S. Y. Huang\n",
        "# Date: 01 Aug 2020\n",
        "# Title: Custom Transformer that can be fitted into Pipeline\n",
        "# URL: https://csyhuang.github.io/2020/08/01/custom-transformer/\n",
        "#############################################################\n",
        "\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "\n",
        "\n",
        "class Stemmer(Transformer, \n",
        "                 HasInputCol, \n",
        "                 HasOutputCol,\n",
        "                 DefaultParamsReadable, \n",
        "                 DefaultParamsWritable):\n",
        "    \n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        super(Stemmer, self).__init__()\n",
        "        kwargs = self._input_kwargs\n",
        "        self.set_params(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def set_params(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        kwargs = self._input_kwargs\n",
        "        self._set(**kwargs)\n",
        "    \n",
        "    def get_input_col(self):\n",
        "        return self.getOrDefault(self.inputCol)\n",
        "\n",
        "    def get_output_col(self):\n",
        "        return self.getOrDefault(self.outputCol)\n",
        "\n",
        "    def _transform(self, df):\n",
        "\n",
        "        # Input and output column\n",
        "        input_col = self.get_input_col()\n",
        "        output_col = self.get_output_col()\n",
        "\n",
        "        # Initialize stemmer from nltk package\n",
        "        ps = PorterStemmer()\n",
        "        \n",
        "        # User Defined Function: stemming every word in the input column\n",
        "        transform_udf = F.udf(lambda x: [ps.stem(word) for word in x], ArrayType(StringType(), False))\n",
        "\n",
        "        # Return the new df with the new column\n",
        "        return df.withColumn(output_col, transform_udf(input_col))\n",
        "\n",
        "# Sanity check\n",
        "# words = Tokenizer(inputCol=\"text\", outputCol=\"words\").transform(spark_df)\n",
        "# test = Stem(inputCol = \"words\", outputCol = \"test\").transform(words)\n",
        "# test.select([\"words\", \"test\"]).show(4)"
      ],
      "metadata": {
        "id": "vJNIKC-uuSDr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Classifier"
      ],
      "metadata": {
        "id": "vwHHeQEpSOWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dummy = (spark_df\n",
        "                .withColumn(\"prediction\", \n",
        "                    when((col(\"author\") == \"NaN\") | (col(\"title\") == \"NaN\") , 1.0)\n",
        "                    .otherwise(0.0))\n",
        "                .withColumn(\"label\", col(\"label\").cast(FloatType()))\n",
        "                .select([\"label\",\"prediction\"])\n",
        "            )\n",
        "\n",
        "df_dummy.show(7)\n",
        "\n",
        "# Sanity Check\n",
        "evaluate(df_dummy, predCol = \"prediction\")"
      ],
      "metadata": {
        "id": "w_8RGzpp14a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b9138e-5139-4328-fc96-a691e10650c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  1.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       0.0|\n",
            "|  1.0|       0.0|\n",
            "|  1.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "+-----+----------+\n",
            "only showing top 7 rows\n",
            "\n",
            "Accuracy: 0.618\n",
            "Recall: 0.567\n",
            "Precision: 0.997\n",
            "F1 Score: 0.723\n",
            "+----------------+-----+----+\n",
            "|label\\prediction|  0.0| 1.0|\n",
            "+----------------+-----+----+\n",
            "|             0.0|10361|  26|\n",
            "|             1.0| 7924|2489|\n",
            "+----------------+-----+----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[10361, 26], [7924, 2489]], 0.9974968710888611, 0.5666393218485097)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning"
      ],
      "metadata": {
        "id": "Ew9QgSIg4xO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Data"
      ],
      "metadata": {
        "id": "HSJKOy1lHgFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data to train and test\n",
        "train, test = df_combined.randomSplit([0.7,0.3], seed=2)"
      ],
      "metadata": {
        "id": "-OcobknvOhr8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "O5F1Qg7QHk7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOTE:*** Fitting the pipeline on the train set takes a while, so I ran it and saved it to a folder \"pipeline\" within the folder \"model and pipeline\". To test it, you can load the pipeline model and use it to transform the test and train dataframes in [Load Pipeline](#load). "
      ],
      "metadata": {
        "id": "qOMo7e4nHUS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline Stages\n",
        "Uncomment the below code to run define stages, and fit pipeline. If loading pipeline from the saved model, then skip this step."
      ],
      "metadata": {
        "id": "1ljBUIqJW-lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Stages for pipeline \n",
        "\n",
        "# # Stage 1 - Tokenizing words\n",
        "# tokenizer = Tokenizer(inputCol=\"full_text\", outputCol=\"full_text_words\")\n",
        "\n",
        "# # Stage 2 - Removing stop words (using nltk stop words)\n",
        "# word_remover = StopWordsRemover(stopWords = stopwords_ls,\n",
        "#                                 inputCol = \"full_text_words\",\n",
        "#                                 outputCol = \"full_text_words_clean\")\n",
        "\n",
        "# # Stage 3 - Lemmatizing each word using custom lemmatizer class\n",
        "# stemmer = Stemmer(inputCol = \"full_text_words_clean\", outputCol = \"stemmed\")\n",
        "\n",
        "# # Stage 4 - Term Frequency of every word\n",
        "# tf = CountVectorizer(inputCol=\"stemmed\", outputCol=\"features\", vocabSize = 1e6)\n",
        "\n",
        "# pipeline = Pipeline(stages= [tokenizer, word_remover, stemmer, tf]).fit(train)\n",
        "# train_df = pipeline.transform(train).select([\"full_text\",\"features\",\"label\"])\n",
        "# test_df = pipeline.transform(test).select([\"full_text\",\"features\",\"label\"])"
      ],
      "metadata": {
        "id": "8j6adcN_2jxl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save Pipeline"
      ],
      "metadata": {
        "id": "jZfg8xovHAZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # #### Save Pipeline! ####\n",
        "# pipeline.save(\"./DSCI632-Project/model and pipeline/pipeline\")"
      ],
      "metadata": {
        "id": "2-ivxz6W4L_C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"load\"></a>\n",
        "#### Load Pipeline "
      ],
      "metadata": {
        "id": "YLMfpwOOHDKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = PipelineModel.load(\"./DSCI632-Project/model and pipeline/pipeline\")\n",
        "train_df = pipeline.transform(train).select([\"full_text\",\"features\",\"label\"])\n",
        "test_df = pipeline.transform(test).select([\"full_text\",\"features\",\"label\"])"
      ],
      "metadata": {
        "id": "PRpxhgqaG-60"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "P9w04Zf45Wj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOTE:*** Fitting Naive Bayes model takes a while. I ran it and saved the model to a folder \"Naive Bayes Model\" within the folder \"model and pipeline\". \n",
        "To test it, skip the code under *Fit Naive Bayes*, and run [*Load Naive Bayes*](#load-na) instead."
      ],
      "metadata": {
        "id": "Ep-8qRpxI3Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit Naive Bayes"
      ],
      "metadata": {
        "id": "R1btxu--JkVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.classification import NaiveBayes\n",
        "# nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", thresholds = [0.6, 0.4])\n",
        "# nb_model = nb.fit(train_df)\n",
        "# predictions_nb = nb_model.transform(test_df)"
      ],
      "metadata": {
        "id": "iabHMfw_48gs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"load-na\"></a>\n",
        "#### Load Naive Bayes"
      ],
      "metadata": {
        "id": "_TJUNNnRJthY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = NaiveBayesModel.load(\"./DSCI632-Project/model and pipeline/Naive Bayes Model\")\n",
        "predictions_nb = nb_model.transform(test_df)\n",
        "predictions_nb.select([\"label\", \"prediction\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-G-LR6_-9av",
        "outputId": "16df5b69-812c-4705-91b1-76c25cf0cbec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       0.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "|  1.0|       1.0|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save Naive Bayes"
      ],
      "metadata": {
        "id": "kEQzPl04KKIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nb_model.save(\"./DSCI632-Project/model and pipeline/Naive Bayes Model\")"
      ],
      "metadata": {
        "id": "naxUrJ6dsWBj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "sDJnz7uE5Z-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(predictions_nb.select([\"label\",\"prediction\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLJlkWQ4TrZY",
        "outputId": "049c57e4-a4ee-41e6-a3cf-adb6a50e90bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.921\n",
            "Recall: 0.885\n",
            "Precision: 0.973\n",
            "F1 Score: 0.927\n",
            "+----------------+----+----+\n",
            "|label\\prediction| 0.0| 1.0|\n",
            "+----------------+----+----+\n",
            "|             0.0|2985|  84|\n",
            "|             1.0| 387|2540|\n",
            "+----------------+----+----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[2985, 84], [387, 2540]], 0.9726295210166178, 0.8852313167259787)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_dummy_rdd = df_dummy.select([\"label\", \"prediction\"]).rdd.map(tuple)\n",
        "predictions_nb_rdd = predictions_nb.select([\"label\", \"prediction\"]).rdd.map(tuple)\n",
        "\n",
        "# Instantiate metrics object\n",
        "metrics_dummy = BinaryClassificationMetrics(predictions_dummy_rdd)\n",
        "metrics_nb = BinaryClassificationMetrics(predictions_nb_rdd)\n",
        "\n",
        "print(\"Area under ROC Baseline: \" , metrics_dummy.areaUnderROC)\n",
        "print(\"Area under ROC Naive Bayes: \" , metrics_nb.areaUnderROC)"
      ],
      "metadata": {
        "id": "Qrj3X854mjif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af61d4c-37a6-4db2-b7ee-e4c80cbc6238"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area under ROC Baseline:  0.7781506748407558\n",
            "Area under ROC Naive Bayes:  0.9266095608020137\n"
          ]
        }
      ]
    }
  ]
}